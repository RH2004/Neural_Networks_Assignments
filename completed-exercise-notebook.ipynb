{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1 computer assignments\n",
    "\n",
    "Fill in the parts labeled **your solution here** and replace *...* with your code. You *do not* need to strictly follow the template, but you may lose points if you do not provide the required results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Basic definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_22247/76233630.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# we start by defining a neural network class that can use different activation functions\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, activation_type='tanh'):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        \n",
    "        # Three fully connected layers with 5 neurons in each layer\n",
    "        self.fc1 = nn.Linear(5, 5)\n",
    "        self.fc2 = nn.Linear(5, 5)\n",
    "        self.fc3 = nn.Linear(5, 5)\n",
    "        \n",
    "        # Set activation function based on input parameter\n",
    "        if activation_type == 'tanh':\n",
    "            self.activation = nn.Tanh()\n",
    "        elif activation_type == 'relu':\n",
    "            self.activation = nn.ReLU()\n",
    "        elif activation_type == 'linear':\n",
    "            self.activation = nn.Identity()  # Linear activation is just identity function\n",
    "        else:\n",
    "            raise ValueError(f\"Activation type '{activation_type}' not supported\")\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Apply layers with the specified activation function\n",
    "        x = self.activation(self.fc1(x))\n",
    "        x = self.activation(self.fc2(x))\n",
    "        x = self.fc3(x)  # No activation on the final layer as per assignment\n",
    "        return x\n",
    "\n",
    "# Create the three neural networks with different activation functions\n",
    "nn_tanh = NeuralNetwork('tanh')\n",
    "nn_relu = NeuralNetwork('relu')\n",
    "nn_linear = NeuralNetwork('linear')\n",
    "\n",
    "# Print the model architectures to verify\n",
    "print(\"Neural Network with tanh activation:\")\n",
    "print(nn_tanh)\n",
    "print(\"\\nNeural Network with ReLU activation:\")\n",
    "print(nn_relu)\n",
    "print(\"\\nNeural Network with linear activation:\")\n",
    "print(nn_linear)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate three sets of random weights for each network\n",
    "def init_random_weights(model):\n",
    "    # Initialize with random weights\n",
    "    for m in model.modules():\n",
    "        if isinstance(m, nn.Linear):\n",
    "            nn.init.xavier_uniform_(m.weight)\n",
    "            nn.init.zeros_(m.bias)\n",
    "    return model\n",
    "\n",
    "# Create copies of networks with different random initializations\n",
    "models = {\n",
    "    'tanh': [init_random_weights(NeuralNetwork('tanh')) for _ in range(3)],\n",
    "    'relu': [init_random_weights(NeuralNetwork('relu')) for _ in range(3)],\n",
    "    'linear': [init_random_weights(NeuralNetwork('linear')) for _ in range(3)]\n",
    "}\n",
    "\n",
    "# Create input data - x1 varies from -10.0 to 10.0, other dimensions are random\n",
    "x1_values = torch.linspace(-10.0, 10.0, 100)\n",
    "x_fixed = torch.randn(4)  # Random values for x2, x3, x4, x5\n",
    "\n",
    "# Prepare to collect outputs\n",
    "activation_types = ['tanh', 'relu', 'linear']\n",
    "all_y1_values = {}\n",
    "\n",
    "# Collect output y1 for each model\n",
    "for act_type in activation_types:\n",
    "    all_y1_values[act_type] = []\n",
    "    for model_idx in range(3):\n",
    "        model = models[act_type][model_idx]\n",
    "        y1_values = []\n",
    "        \n",
    "        # For each x1 value, create full input and get y1 output\n",
    "        for x1 in x1_values:\n",
    "            x_full = torch.cat([x1.reshape(1), x_fixed]).reshape(1, 5)\n",
    "            with torch.no_grad():\n",
    "                output = model(x_full)\n",
    "                y1 = output[0, 0].item()  # Get the first element of output\n",
    "                y1_values.append(y1)\n",
    "        \n",
    "        all_y1_values[act_type].append(y1_values)\n",
    "\n",
    "# Code for reporting the 9 plots:\n",
    "fig, axs = plt.subplots(nrows=3, ncols=3, figsize=(15, 10))\n",
    "for i in range(3): # loop over rows of plot (activation types)\n",
    "    act_type = activation_types[i]\n",
    "    for j in range(3): # loop over cols of plot (weight sets)\n",
    "        y1_values = all_y1_values[act_type][j]\n",
    "        axs[i, j].plot(x1_values.numpy(), y1_values)\n",
    "        axs[i, j].set_title(f'{act_type} activation, weight set {j+1}')\n",
    "        axs[i, j].set_xlabel('x1')\n",
    "        axs[i, j].set_ylabel('y1')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function f(w) as given in the problem\n",
    "def f(w):\n",
    "    \"\"\"Function f(w) as defined in equation (6) of the assignment\n",
    "    \n",
    "    Args:\n",
    "        w: Input vector of shape (2,)\n",
    "        \n",
    "    Returns:\n",
    "        The function value f(w)\n",
    "    \"\"\"\n",
    "    term1 = np.exp(-w[0]**2 - 2*(w[1] - 1)**2)\n",
    "    term2 = np.exp(-(w[0] - 1)**2 - 2*w[1]**2)\n",
    "    return term1 + term2\n",
    "\n",
    "# Define the gradient of f(w)\n",
    "def grad_f(w):\n",
    "    \"\"\"Gradient of the function f(w)\n",
    "    \n",
    "    Args:\n",
    "        w: Input vector of shape (2,)\n",
    "        \n",
    "    Returns:\n",
    "        Gradient of f(w), shape (2,)\n",
    "    \"\"\"\n",
    "    # Gradient components for the first term\n",
    "    term1 = np.exp(-w[0]**2 - 2*(w[1] - 1)**2)\n",
    "    grad_term1_w1 = -2*w[0] * term1\n",
    "    grad_term1_w2 = -4*(w[1] - 1) * term1\n",
    "    \n",
    "    # Gradient components for the second term\n",
    "    term2 = np.exp(-(w[0] - 1)**2 - 2*w[1]**2)\n",
    "    grad_term2_w1 = -2*(w[0] - 1) * term2\n",
    "    grad_term2_w2 = -4*w[1] * term2\n",
    "    \n",
    "    # Sum the gradients\n",
    "    grad_w1 = grad_term1_w1 + grad_term2_w1\n",
    "    grad_w2 = grad_term1_w2 + grad_term2_w2\n",
    "    \n",
    "    return np.array([grad_w1, grad_w2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code for plotting isocontours of the function f\n",
    "vec_f = np.vectorize(f, signature=\"(n)->()\")\n",
    "w1 = np.linspace(-2.0, 3.0, 200)\n",
    "w2 = np.linspace(-2.0, 3.0, 200)\n",
    "W1, W2 = np.meshgrid(w1, w2)\n",
    "Fs = vec_f(np.stack((W1, W2), axis=2))\n",
    "plt.figure(dpi=150)\n",
    "plt.contour(W1, W2, Fs)\n",
    "plt.xlabel(\"w1\")\n",
    "plt.ylabel(\"w2\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The gradient of the function f(w) = exp(-w₁² - 2(w₂ - 1)²) + exp(-(w₁ - 1)² - 2w₂²) is calculated as follows:**\n",
    "\n",
    "For the first term, f₁(w) = exp(-w₁² - 2(w₂ - 1)²):\n",
    "\n",
    "∂f₁/∂w₁ = ∂/∂w₁[exp(-w₁² - 2(w₂ - 1)²)] = -2w₁ · exp(-w₁² - 2(w₂ - 1)²)\n",
    "\n",
    "∂f₁/∂w₂ = ∂/∂w₂[exp(-w₁² - 2(w₂ - 1)²)] = -4(w₂ - 1) · exp(-w₁² - 2(w₂ - 1)²)\n",
    "\n",
    "For the second term, f₂(w) = exp(-(w₁ - 1)² - 2w₂²):\n",
    "\n",
    "∂f₂/∂w₁ = ∂/∂w₁[exp(-(w₁ - 1)² - 2w₂²)] = -2(w₁ - 1) · exp(-(w₁ - 1)² - 2w₂²)\n",
    "\n",
    "∂f₂/∂w₂ = ∂/∂w₂[exp(-(w₁ - 1)² - 2w₂²)] = -4w₂ · exp(-(w₁ - 1)² - 2w₂²)\n",
    "\n",
    "The complete gradient of f(w) is:\n",
    "\n",
    "∇f(w) = [∂f₁/∂w₁ + ∂f₂/∂w₁, ∂f₁/∂w₂ + ∂f₂/∂w₂]\n",
    "\n",
    "= [-2w₁ · exp(-w₁² - 2(w₂ - 1)²) - 2(w₁ - 1) · exp(-(w₁ - 1)² - 2w₂²),\n",
    "   -4(w₂ - 1) · exp(-w₁² - 2(w₂ - 1)²) - 4w₂ · exp(-(w₁ - 1)² - 2w₂²)]\n",
    "\n",
    "This is what I implemented in the `grad_f` function above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1 = np.asarray([0.2, 0.5])\n",
    "w2 = np.asarray([0.5, 0.2])\n",
    "w3 = np.asarray([1.0, 1.0])\n",
    "\n",
    "# Implement gradient ascent (since we want to maximize)\n",
    "def gradient_ascent(start_point, learning_rate, max_iterations=1000, tolerance=1e-4):\n",
    "    \"\"\"\n",
    "    Performs gradient ascent to maximize f(w)\n",
    "    \n",
    "    Args:\n",
    "        start_point: Initial point, shape (2,)\n",
    "        learning_rate: Step size for gradient ascent\n",
    "        max_iterations: Maximum number of iterations\n",
    "        tolerance: Stopping criterion based on gradient norm\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (final_point, trajectory, function_values)\n",
    "    \"\"\"\n",
    "    w = start_point.copy()\n",
    "    trajectory = [w.copy()]\n",
    "    function_values = [f(w)]\n",
    "    \n",
    "    for i in range(max_iterations):\n",
    "        # Compute gradient\n",
    "        gradient = grad_f(w)\n",
    "        \n",
    "        # Check stopping criterion\n",
    "        if np.linalg.norm(gradient) < tolerance:\n",
    "            break\n",
    "        \n",
    "        # Update weights (ascent, so we add the gradient)\n",
    "        w = w + learning_rate * gradient\n",
    "        \n",
    "        # Store trajectory and function value\n",
    "        trajectory.append(w.copy())\n",
    "        function_values.append(f(w))\n",
    "        \n",
    "    return w, np.array(trajectory), np.array(function_values)\n",
    "\n",
    "# Try different learning rates for each starting point to find optimal ones\n",
    "learning_rates = [0.05, 0.1, 0.2]\n",
    "best_results = []\n",
    "\n",
    "for i, start_point in enumerate([w1, w2, w3]):\n",
    "    best_f_val = -np.inf\n",
    "    best_result = None\n",
    "    \n",
    "    # Try different learning rates\n",
    "    for lr in learning_rates:\n",
    "        final_w, trajectory, f_values = gradient_ascent(start_point, lr)\n",
    "        if f_values[-1] > best_f_val:\n",
    "            best_f_val = f_values[-1]\n",
    "            best_result = (final_w, trajectory, f_values, lr)\n",
    "    \n",
    "    best_results.append(best_result)\n",
    "    print(f\"Starting point {i+1}: {start_point}\")\n",
    "    print(f\"  Best learning rate: {best_result[3]}\")\n",
    "    print(f\"  Final point: {best_result[0]}\")\n",
    "    print(f\"  Final function value: {best_result[2][-1]}\")\n",
    "    print(f\"  Number of iterations: {len(best_result[2])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Extract trajectories and learning rates for plotting\n",
    "trajectories = [result[1] for result in best_results]\n",
    "function_values = [result[2] for result in best_results]\n",
    "learning_rates = [result[3] for result in best_results]\n",
    "\n",
    "# Code for plotting.\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5), dpi=150)\n",
    "for i, trajectory in enumerate(trajectories):\n",
    "    # Plot contour and trajectory for each starting point\n",
    "    ax = axes[i]\n",
    "    ax.contour(W1, W2, Fs)\n",
    "    ax.plot(trajectory[0, 0], trajectory[0, 1], marker=\"o\", color=\"blue\", label=\"Initial\")\n",
    "    for j in range(len(trajectory)-1):\n",
    "        ax.arrow(trajectory[j, 0], trajectory[j, 1], \n",
    "                trajectory[j+1, 0] - trajectory[j, 0], \n",
    "                trajectory[j+1, 1] - trajectory[j, 1], \n",
    "                length_includes_head=True, width=0.03)\n",
    "    ax.plot(trajectory[-1, 0], trajectory[-1, 1], marker=\"o\", color=\"red\", label=\"Final\")\n",
    "    ax.set_xlabel(\"w1\")\n",
    "    ax.set_ylabel(\"w2\")\n",
    "    ax.legend()\n",
    "    ax.set_title(f\"Start: {[w1, w2, w3][i]}, LR: {learning_rates[i]}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code for plotting.\n",
    "fig, axs = plt.subplots(1, 3, figsize=(15, 5), sharey=True)\n",
    "for i, f_values in enumerate(function_values):\n",
    "    axs[i].plot(f_values)\n",
    "    axs[i].set_xlabel(\"Number of iterations\")\n",
    "    axs[i].set_ylabel(\"Function value\")\n",
    "    axs[i].set_title(f\"Starting from {['w1', 'w2', 'w3'][i]}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discussion on local vs. global maxima:**\n",
    "\n",
    "From the optimization results, we can observe several important characteristics:\n",
    "\n",
    "1. The function f(w) has at least two distinct local maxima, as evidenced by the different convergence points from our three starting locations.\n",
    "\n",
    "2. Starting points w1 = [0.2, 0.5] and w2 = [0.5, 0.2] converged to different local maxima. This demonstrates how gradient-based methods are sensitive to initialization and can get trapped in local optima.\n",
    "\n",
    "3. From the function value plots, we can see that different starting points achieved different maximum values, suggesting that one of these is the global maximum while others are local maxima.\n",
    "\n",
    "4. The contour plots clearly show the two \"hills\" in the function landscape, confirming the presence of multiple local maxima.\n",
    "\n",
    "5. Gradient ascent, like other local optimization methods, cannot guarantee finding the global optimum unless the function is convex (which this one is not).\n",
    "\n",
    "This exercise demonstrates why in practice, we often use multiple random initializations when optimizing non-convex functions, to increase the chances of finding the global optimum. It also highlights the limitation of gradient-based methods for non-convex optimization problems, which is relevant to neural network training where the loss landscape typically contains many local optima."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
